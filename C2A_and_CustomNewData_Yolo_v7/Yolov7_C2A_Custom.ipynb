{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPjtDHMs9qcvv69ixrinzIj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyMoYMLzIzGq","executionInfo":{"status":"ok","timestamp":1732986011168,"user_tz":-330,"elapsed":2487,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}},"outputId":"32477a73-24ae-495f-8208-44054f04f1bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["!git clone https://github.com/WongKinYiu/yolov7.git       # clone\n","%cd yolov7\n","!pip install -r requirements.txt      # install modules\n","!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt # download pretrained weight"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7UnEMVVS_FX","executionInfo":{"status":"ok","timestamp":1732986008686,"user_tz":-330,"elapsed":7171,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}},"outputId":"7791dea4-d214-4116-90f1-a06ab17c7c98"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'yolov7' already exists and is not an empty directory.\n","/content/yolov7\n","Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.8.0)\n","Requirement already satisfied: numpy<1.24.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.23.5)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.10.0.84)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (11.0.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.13.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.5.1+cu121)\n","Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.20.1+cu121)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.66.6)\n","Requirement already satisfied: protobuf<4.21.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (4.21.2)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.17.1)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (0.13.2)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (7.34.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (5.9.5)\n","Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 36)) (0.1.1.post2209072238)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2024.8.30)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.3.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.68.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.7)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (75.1.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.1.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2024.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (3.0.48)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 34)) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 34)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 34)) (0.2.13)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.0.2)\n","--2024-11-30 17:00:07--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241130T170007Z&X-Amz-Expires=300&X-Amz-Signature=0332cf6d73db2b34be67725ed5fd0c67f6cb0f046ff2aafdcbe684f958c55bc8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream [following]\n","--2024-11-30 17:00:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241130T170007Z&X-Amz-Expires=300&X-Amz-Signature=0332cf6d73db2b34be67725ed5fd0c67f6cb0f046ff2aafdcbe684f958c55bc8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 75587165 (72M) [application/octet-stream]\n","Saving to: â€˜yolov7.pt.1â€™\n","\n","yolov7.pt.1         100%[===================>]  72.08M   306MB/s    in 0.2s    \n","\n","2024-11-30 17:00:07 (306 MB/s) - â€˜yolov7.pt.1â€™ saved [75587165/75587165]\n","\n"]}]},{"cell_type":"code","source":["%cd /content/yolov7/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLC7xQxHUsBz","executionInfo":{"status":"ok","timestamp":1732986166053,"user_tz":-330,"elapsed":351,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}},"outputId":"636782fa-0966-4985-d393-136a4d6d5c22"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov7\n"]}]},{"cell_type":"code","source":["!python train.py --weights yolov7.pt --data \"/content/drive/MyDrive/C2A_Dataset/data_C2A.yaml\" --workers 8 --batch-size 8 --img 416 --cfg cfg/training/yolov7.yaml --name yolov7 --epochs 2 --hyp data/hyp.scratch.p5.yaml\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JlZtgX45WZHf","executionInfo":{"status":"ok","timestamp":1732990947412,"user_tz":-330,"elapsed":4526183,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}},"outputId":"75e2fe7f-6344-4863-d62e-ce92907552f5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-30 17:07:04.522588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-30 17:07:04.542018: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-30 17:07:04.549537: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-30 17:07:04.563803: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-30 17:07:05.561288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","YOLOR ðŸš€ v0.1-128-ga207844 torch 2.5.1+cu121 CUDA:0 (Tesla T4, 15102.0625MB)\n","\n","Namespace(weights='yolov7.pt', cfg='cfg/training/yolov7.yaml', data='/content/drive/MyDrive/C2A_Dataset/data_C2A.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=2, batch_size=8, img_size=[416, 416], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='yolov7', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/yolov7', total_batch_size=8)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n","/content/yolov7/train.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  run_id = torch.load(weights, map_location=device).get('wandb_id') if weights.endswith('.pt') and os.path.isfile(weights) else None\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id t27zjm4r.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n","/content/yolov7/train.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(weights, map_location=device)  # load checkpoint\n","Overriding model.yaml nc=80 with nc=1\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 12                -1  1         0  models.common.MP                        []                            \n"," 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n"," 25                -1  1         0  models.common.MP                        []                            \n"," 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 38                -1  1         0  models.common.MP                        []                            \n"," 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n"," 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n"," 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n"," 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 76                -1  1         0  models.common.MP                        []                            \n"," 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n"," 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 89                -1  1         0  models.common.MP                        []                            \n"," 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n"," 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n"," 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n","100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n","101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n","102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n","103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n","104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n","105   [102, 103, 104]  1     34156  models.yolo.IDetect                     [1, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 415 layers, 37196556 parameters, 37196556 gradients, 105.1 GFLOPS\n","\n","Transferred 552/566 items from yolov7.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/drive/MyDrive/C2A_Dataset/C2A_Dataset/new_dataset3/train/labels' images and labels... 4274 found, 0 missing, 0 empty, 0 corrupted:  70% 4274/6129 [30:18<12:44,  2.43it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/drive/MyDrive/C2A_Dataset/C2A_Dataset/new_dataset3/train/images/flood_image0407_3.png: duplicate labels\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/drive/MyDrive/C2A_Dataset/C2A_Dataset/new_dataset3/train/labels' images and labels... 6129 found, 0 missing, 0 empty, 1 corrupted: 100% 6129/6129 [42:12<00:00,  2.42it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/C2A_Dataset/C2A_Dataset/new_dataset3/train/labels.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/YOLOv_Dataset/val/labels' images and labels... 50 found, 0 missing, 0 empty, 0 corrupted: 100% 50/50 [00:38<00:00,  1.29it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/YOLOv_Dataset/val/labels.cache\n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.30, Best Possible Recall (BPR) = 0.8115. Attempting to improve anchors, please wait...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 34890 of 215785 labels are < 3 pixels in size.\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 211756 points...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9866 best possible recall, 5.62 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=416, metric_all=0.358/0.728-mean/best, past_thr=0.487-mean: 3,4,  5,9,  12,8,  9,16,  20,13,  15,26,  32,20,  27,42,  51,28\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7539: 100% 1000/1000 [01:01<00:00, 16.26it/s]\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9930 best possible recall, 5.87 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=416, metric_all=0.372/0.746-mean/best, past_thr=0.491-mean: 2,4,  4,5,  9,7,  6,11,  13,12,  10,19,  27,15,  19,25,  39,27\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n","\n","/content/yolov7/train.py:299: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = amp.GradScaler(enabled=cuda)\n","Image sizes 416 train, 416 test\n","Using 2 dataloader workers\n","Logging results to runs/train/yolov7\n","Starting training for 2 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","  0% 0/766 [00:00<?, ?it/s]/content/yolov7/train.py:360: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with amp.autocast(enabled=cuda):\n","       0/1     3.53G   0.07749   0.02859         0    0.1061       353       416: 100% 766/766 [15:10<00:00,  1.19s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:05<00:00,  1.41s/it]\n","                 all          50        1000       0.407       0.387        0.29      0.0662\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       1/1     3.52G   0.05165   0.03657         0   0.08823       382       416: 100% 766/766 [14:29<00:00,  1.14s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:03<00:00,  1.03it/s]\n","                 all          50        1000       0.451       0.365       0.332      0.0925\n","Images sizes do not match. This will causes images to be display incorrectly in the UI.\n","2 epochs completed in 0.503 hours.\n","\n","/content/yolov7/utils/general.py:802: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  x = torch.load(f, map_location=torch.device('cpu'))\n","Optimizer stripped from runs/train/yolov7/weights/last.pt, 74.8MB\n","Optimizer stripped from runs/train/yolov7/weights/best.pt, 74.8MB\n","Images sizes do not match. This will causes images to be display incorrectly in the UI.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss â–â–\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss â–â–\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 0.3316\n","\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 0.09248\n","\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision 0.45107\n","\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall 0.365\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss 0.05165\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss 0\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss 0.03657\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss 0.11004\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss 0\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss 0.0526\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 0.00366\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 0.00366\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 0.03704\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/yolov7/wandb/offline-run-20241130_170725-t27zjm4r\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20241130_170725-t27zjm4r/logs\u001b[0m\n"]}]},{"cell_type":"code","source":["!python test.py --data /content/drive/MyDrive/C2A_Dataset/data_C2A.yaml --img 416 --batch 8 --conf 0.35 --iou 0.65 --device 0 --weights /content/yolov7/runs/train/yolov7/weights/best.pt --name yolov7_416_val\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aDmIKIjlaKT3","executionInfo":{"status":"ok","timestamp":1732991278739,"user_tz":-330,"elapsed":20941,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}},"outputId":"cf3c915a-361b-4193-e2d5-90837254bde0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(weights=['/content/yolov7/runs/train/yolov7/weights/best.pt'], data='/content/drive/MyDrive/C2A_Dataset/data_C2A.yaml', batch_size=8, img_size=416, conf_thres=0.35, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='yolov7_416_val', exist_ok=False, no_trace=False, v5_metric=False)\n","YOLOR ðŸš€ v0.1-128-ga207844 torch 2.5.1+cu121 CUDA:0 (Tesla T4, 15102.0625MB)\n","\n","/content/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(w, map_location=map_location)  # load\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 314 layers, 36481772 parameters, 6194944 gradients, 103.2 GFLOPS\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","/content/yolov7/utils/datasets.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  cache, exists = torch.load(cache_path), True  # load\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/YOLOv_Dataset/val/labels.cache' images and labels... 50 found, 0 missing, 0 empty, 0 corrupted: 100% 50/50 [00:00<?, ?it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 7/7 [00:04<00:00,  1.59it/s]\n","                 all          50        1000       0.667       0.172       0.144      0.0431\n","Speed: 31.9/27.6/59.5 ms inference/NMS/total per 416x416 image at batch-size 8\n","Results saved to runs/test/yolov7_416_val2\n"]}]},{"cell_type":"code","source":["!python detect.py --weights /content/yolov7/runs/train/yolov7/weights/best.pt --source \"/content/drive/MyDrive/YOLOv_Dataset/test/images\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zurL4Z_JiKQQ","executionInfo":{"status":"ok","timestamp":1732991356340,"user_tz":-330,"elapsed":45137,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}},"outputId":"fbe19077-02dc-4d8f-b282-1fb1b47ca86f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(weights=['/content/yolov7/runs/train/yolov7/weights/best.pt'], source='/content/drive/MyDrive/YOLOv_Dataset/test/images', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n","YOLOR ðŸš€ v0.1-128-ga207844 torch 2.5.1+cu121 CUDA:0 (Tesla T4, 15102.0625MB)\n","\n","/content/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(w, map_location=map_location)  # load\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 314 layers, 36481772 parameters, 6194944 gradients, 103.2 GFLOPS\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","4 persons, Done. (22.1ms) Inference, (464.8ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_139.jpg\n","24 persons, Done. (22.2ms) Inference, (1.7ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_157.jpg\n","5 persons, Done. (22.1ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_161.jpg\n","10 persons, Done. (21.1ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_191.jpg\n","7 persons, Done. (20.9ms) Inference, (1.8ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_192.jpg\n","16 persons, Done. (20.0ms) Inference, (1.7ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_197.jpg\n","3 persons, Done. (20.2ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_2.jpg\n","2 persons, Done. (20.0ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_214.jpg\n","5 persons, Done. (19.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_231.jpg\n","4 persons, Done. (19.8ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_252.jpg\n","12 persons, Done. (21.9ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_267.jpg\n","4 persons, Done. (21.8ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_277.jpg\n","31 persons, Done. (22.1ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_283.jpg\n","7 persons, Done. (20.2ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_29.jpg\n","6 persons, Done. (19.3ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_292.jpg\n","7 persons, Done. (15.1ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_294.jpg\n","10 persons, Done. (21.2ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_302.jpg\n","1 person, Done. (20.4ms) Inference, (1.6ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_304.jpg\n","4 persons, Done. (20.1ms) Inference, (2.8ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_309.jpg\n","4 persons, Done. (20.1ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_311.jpg\n","6 persons, Done. (20.1ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_323.jpg\n","4 persons, Done. (21.9ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_326.jpg\n","4 persons, Done. (19.6ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_328.jpg\n","4 persons, Done. (13.9ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_335.jpg\n","7 persons, Done. (14.0ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_337.jpg\n","5 persons, Done. (21.9ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_340.jpg\n","7 persons, Done. (15.0ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_341.jpg\n","4 persons, Done. (21.2ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_352.jpg\n","8 persons, Done. (14.1ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_355.jpg\n","6 persons, Done. (20.8ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_37.jpg\n","5 persons, Done. (20.2ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_38.jpg\n","12 persons, Done. (20.0ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_401.jpg\n","1 person, Done. (20.1ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_406.jpg\n","6 persons, Done. (21.7ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_422.jpg\n","6 persons, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_439.jpg\n","1 person, Done. (22.1ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_442.jpg\n","5 persons, Done. (20.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_443.jpg\n","4 persons, Done. (19.6ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_448.jpg\n","7 persons, Done. (13.9ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_466.jpg\n","8 persons, Done. (13.9ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_469.jpg\n","9 persons, Done. (16.6ms) Inference, (1.9ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_498.jpg\n","7 persons, Done. (15.0ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_499.jpg\n","5 persons, Done. (22.1ms) Inference, (1.7ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_50.jpg\n","8 persons, Done. (22.2ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_57.jpg\n","6 persons, Done. (21.2ms) Inference, (1.7ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_58.jpg\n","6 persons, Done. (22.1ms) Inference, (1.8ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_70.jpg\n","9 persons, Done. (20.6ms) Inference, (1.4ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_81.jpg\n","9 persons, Done. (15.9ms) Inference, (2.9ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_87.jpg\n","4 persons, Done. (13.9ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_94.jpg\n","14 persons, Done. (16.2ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp/combined_image_99.jpg\n","Done. (25.241s)\n"]}]},{"cell_type":"code","source":["# Example: Saving the trained model weights and other results to Google Drive\n","!cp -r /content/yolov7/runs/train/yolov7 /content/drive/MyDrive/CV_results_USD/Yolov9c_C2A_CustomData/"],"metadata":{"id":"q5g-GkW5juxZ","executionInfo":{"status":"ok","timestamp":1732991444999,"user_tz":-330,"elapsed":7099,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["!python train.py --weights /content/yolov7/runs/train/yolov7/weights/best.pt --data \"/content/drive/MyDrive/C2A_Dataset/data.yaml\" --workers 8 --batch-size 8 --img 416 --cfg cfg/training/yolov7.yaml --name yolov7 --epochs 2 --hyp data/hyp.scratch.p5.yaml"],"metadata":{"id":"GtCMK7Fumnbh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732992164390,"user_tz":-330,"elapsed":288510,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}},"outputId":"cba24e41-c8f0-4736-a623-3f81af2f9eff"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-11-30 18:37:57.764923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-30 18:37:57.785205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-30 18:37:57.791458: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-30 18:37:57.806228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-30 18:37:59.709930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","YOLOR ðŸš€ v0.1-128-ga207844 torch 2.5.1+cu121 CUDA:0 (Tesla T4, 15102.0625MB)\n","\n","Namespace(weights='/content/yolov7/runs/train/yolov7/weights/best.pt', cfg='cfg/training/yolov7.yaml', data='/content/drive/MyDrive/C2A_Dataset/data.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=2, batch_size=8, img_size=[416, 416], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='yolov7', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/yolov72', total_batch_size=8)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n","/content/yolov7/train.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  run_id = torch.load(weights, map_location=device).get('wandb_id') if weights.endswith('.pt') and os.path.isfile(weights) else None\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id zggf5ee5.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n","/content/yolov7/train.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(weights, map_location=device)  # load checkpoint\n","Overriding model.yaml nc=80 with nc=1\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 12                -1  1         0  models.common.MP                        []                            \n"," 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n"," 25                -1  1         0  models.common.MP                        []                            \n"," 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 38                -1  1         0  models.common.MP                        []                            \n"," 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n"," 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n"," 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n"," 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 76                -1  1         0  models.common.MP                        []                            \n"," 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n"," 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 89                -1  1         0  models.common.MP                        []                            \n"," 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n"," 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n"," 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n","100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n","101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n","102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n","103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n","104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n","105   [102, 103, 104]  1     34156  models.yolo.IDetect                     [1, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 415 layers, 37196556 parameters, 37196556 gradients, 105.1 GFLOPS\n","\n","Transferred 564/566 items from /content/yolov7/runs/train/yolov7/weights/best.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/drive/MyDrive/YOLOv_Dataset/train/labels' images and labels... 400 found, 0 missing, 0 empty, 0 corrupted: 100% 400/400 [02:39<00:00,  2.51it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/YOLOv_Dataset/train/labels.cache\n","/content/yolov7/utils/datasets.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  cache, exists = torch.load(cache_path), True  # load\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/YOLOv_Dataset/val/labels.cache' images and labels... 50 found, 0 missing, 0 empty, 0 corrupted: 100% 50/50 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.78, Best Possible Recall (BPR) = 0.9075. Attempting to improve anchors, please wait...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 510 of 8000 labels are < 3 pixels in size.\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 7994 points...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9966 best possible recall, 6.00 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=416, metric_all=0.379/0.795-mean/best, past_thr=0.487-mean: 4,7,  10,5,  9,19,  19,9,  28,14,  14,30,  41,19,  22,48,  60,27\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8106: 100% 1000/1000 [00:02<00:00, 413.89it/s]\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9984 best possible recall, 6.24 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=416, metric_all=0.392/0.810-mean/best, past_thr=0.492-mean: 4,8,  8,4,  15,8,  8,16,  23,12,  14,26,  33,17,  21,42,  49,25\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n","\n","/content/yolov7/train.py:299: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = amp.GradScaler(enabled=cuda)\n","Image sizes 416 train, 416 test\n","Using 2 dataloader workers\n","Logging results to runs/train/yolov72\n","Starting training for 2 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","  0% 0/50 [00:00<?, ?it/s]/content/yolov7/train.py:360: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with amp.autocast(enabled=cuda):\n","       0/1     3.43G    0.0603   0.02989         0   0.09019       283       416: 100% 50/50 [00:34<00:00,  1.44it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:05<00:00,  1.49s/it]\n","                 all          50        1000       0.792       0.635       0.708       0.311\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       1/1     3.43G   0.04489    0.0287         0   0.07359       190       416: 100% 50/50 [00:28<00:00,  1.76it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:05<00:00,  1.35s/it]\n","                 all          50        1000       0.841       0.748        0.81       0.432\n","Images sizes do not match. This will causes images to be display incorrectly in the UI.\n","2 epochs completed in 0.026 hours.\n","\n","/content/yolov7/utils/general.py:802: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  x = torch.load(f, map_location=torch.device('cpu'))\n","Optimizer stripped from runs/train/yolov72/weights/last.pt, 74.8MB\n","Optimizer stripped from runs/train/yolov72/weights/best.pt, 74.8MB\n","Images sizes do not match. This will causes images to be display incorrectly in the UI.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss â–â–\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss â–â–\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 â–â–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 0.81023\n","\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 0.4315\n","\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision 0.84051\n","\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall 0.748\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss 0.04489\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss 0\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss 0.0287\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss 0.08779\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss 0\n","\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss 0.06072\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 0.00054\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 0.00054\n","\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 0.09064\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/yolov7/wandb/offline-run-20241130_183818-zggf5ee5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20241130_183818-zggf5ee5/logs\u001b[0m\n"]}]},{"cell_type":"code","source":["!python test.py --data /content/drive/MyDrive/C2A_Dataset/data.yaml --img 416 --batch 8 --conf 0.35 --iou 0.65 --device 0 --weights /content/yolov7/runs/train/yolov72/weights/best.pt --name yolov7_416_val"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6N-dnApFqIB","executionInfo":{"status":"ok","timestamp":1732992552035,"user_tz":-330,"elapsed":20579,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}},"outputId":"d27dc153-e35e-4292-a8b9-a498783578d7"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(weights=['/content/yolov7/runs/train/yolov72/weights/best.pt'], data='/content/drive/MyDrive/C2A_Dataset/data.yaml', batch_size=8, img_size=416, conf_thres=0.35, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='yolov7_416_val', exist_ok=False, no_trace=False, v5_metric=False)\n","YOLOR ðŸš€ v0.1-128-ga207844 torch 2.5.1+cu121 CUDA:0 (Tesla T4, 15102.0625MB)\n","\n","/content/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(w, map_location=map_location)  # load\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 314 layers, 36481772 parameters, 6194944 gradients, 103.2 GFLOPS\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","/content/yolov7/utils/datasets.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  cache, exists = torch.load(cache_path), True  # load\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/YOLOv_Dataset/val/labels.cache' images and labels... 50 found, 0 missing, 0 empty, 0 corrupted: 100% 50/50 [00:00<?, ?it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 7/7 [00:03<00:00,  2.17it/s]\n","                 all          50        1000       0.834        0.74       0.729       0.404\n","Speed: 24.0/13.9/37.8 ms inference/NMS/total per 416x416 image at batch-size 8\n","Results saved to runs/test/yolov7_416_val3\n"]}]},{"cell_type":"code","source":["!python detect.py --weights /content/yolov7/runs/train/yolov72/weights/best.pt --source \"/content/drive/MyDrive/YOLOv_Dataset/test/images\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeeYG70YF9ga","executionInfo":{"status":"ok","timestamp":1732992666151,"user_tz":-330,"elapsed":24276,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}},"outputId":"044968c9-4aa1-4f40-8b4f-09a74fffb66d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(weights=['/content/yolov7/runs/train/yolov72/weights/best.pt'], source='/content/drive/MyDrive/YOLOv_Dataset/test/images', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n","YOLOR ðŸš€ v0.1-128-ga207844 torch 2.5.1+cu121 CUDA:0 (Tesla T4, 15102.0625MB)\n","\n","/content/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(w, map_location=map_location)  # load\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 314 layers, 36481772 parameters, 6194944 gradients, 103.2 GFLOPS\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","25 persons, Done. (22.2ms) Inference, (483.9ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_139.jpg\n","45 persons, Done. (22.2ms) Inference, (1.4ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_157.jpg\n","27 persons, Done. (22.2ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_161.jpg\n","29 persons, Done. (21.1ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_191.jpg\n","27 persons, Done. (15.3ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_192.jpg\n","33 persons, Done. (15.2ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_197.jpg\n","20 persons, Done. (13.2ms) Inference, (1.7ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_2.jpg\n","20 persons, Done. (13.2ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_214.jpg\n","23 persons, Done. (12.7ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_231.jpg\n","27 persons, Done. (12.6ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_252.jpg\n","27 persons, Done. (12.2ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_267.jpg\n","25 persons, Done. (12.2ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_277.jpg\n","52 persons, Done. (12.0ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_283.jpg\n","21 persons, Done. (12.0ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_29.jpg\n","20 persons, Done. (11.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_292.jpg\n","19 persons, Done. (8.4ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_294.jpg\n","20 persons, Done. (11.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_302.jpg\n","20 persons, Done. (11.2ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_304.jpg\n","26 persons, Done. (10.9ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_309.jpg\n","23 persons, Done. (10.8ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_311.jpg\n","17 persons, Done. (10.7ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_323.jpg\n","20 persons, Done. (12.1ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_326.jpg\n","20 persons, Done. (11.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_328.jpg\n","22 persons, Done. (7.3ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_335.jpg\n","23 persons, Done. (7.5ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_337.jpg\n","22 persons, Done. (12.2ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_340.jpg\n","27 persons, Done. (7.9ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_341.jpg\n","19 persons, Done. (11.3ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_352.jpg\n","20 persons, Done. (7.6ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_355.jpg\n","28 persons, Done. (12.1ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_37.jpg\n","27 persons, Done. (12.6ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_38.jpg\n","29 persons, Done. (11.8ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_401.jpg\n","22 persons, Done. (12.3ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_406.jpg\n","19 persons, Done. (11.0ms) Inference, (1.6ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_422.jpg\n","21 persons, Done. (8.8ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_439.jpg\n","34 persons, Done. (12.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_442.jpg\n","25 persons, Done. (12.3ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_443.jpg\n","21 persons, Done. (11.6ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_448.jpg\n","20 persons, Done. (7.8ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_466.jpg\n","21 persons, Done. (7.4ms) Inference, (1.0ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_469.jpg\n","21 persons, Done. (8.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_498.jpg\n","22 persons, Done. (8.1ms) Inference, (1.4ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_499.jpg\n","22 persons, Done. (11.7ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_50.jpg\n","35 persons, Done. (11.7ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_57.jpg\n","22 persons, Done. (11.2ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_58.jpg\n","25 persons, Done. (12.3ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_70.jpg\n","26 persons, Done. (12.6ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_81.jpg\n","29 persons, Done. (9.5ms) Inference, (1.9ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_87.jpg\n","22 persons, Done. (11.0ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_94.jpg\n","31 persons, Done. (8.1ms) Inference, (1.6ms) NMS\n"," The image with the result is saved in: runs/detect/exp2/combined_image_99.jpg\n","Done. (4.340s)\n"]}]},{"cell_type":"code","source":["# Example: Saving the trained model weights and other results to Google Drive\n","!cp -r /content/yolov7/runs /content/drive/MyDrive/CV_results_USD/Yolov9c_C2A_CustomData/"],"metadata":{"id":"9RxcIwmDaSkC","executionInfo":{"status":"ok","timestamp":1732992795097,"user_tz":-330,"elapsed":14567,"user":{"displayName":"Shruthi AK","userId":"17579586896252994635"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"235BvTVDa0am"},"execution_count":null,"outputs":[]}]}